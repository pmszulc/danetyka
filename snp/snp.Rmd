---
title: "Jak znaleźć istotne geny?" 
output:
  github_document: default
  html_document:
    highlight: pygments
editor_options:
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  out.width = "80%",
  fig.width = 8,
  fig.asp = 0.618,
  message = FALSE,
  warning = FALSE
)
library("jpeg")
```

## Geny i SNP

Najpierw krótki wstęp genetyczny, bo bez niego ciężko będzie zrozumieć, o co chodzi w analizowanych przeze mnie danych. Będziemy szukać genów, więc zacznijmy od tego, co ten termin oznacza. Gen możemy rozumieć jako fragment łańcucha DNA, który ma znaczenie (wpływa na jakąś cechę), i który występuje przynajmniej w dwóch wersjach, tak zwanych allelach. W zależności od tego, czy mamy gen w wersji *A* lub *a*, może to skutkować na przykład wyższym ryzykiem zachorowania na pewną chorobę.

Dalej, to, co nas najbardziej interesuje w budowie DNA, to zasady azotowe. Zwykle występują w czterech wersjach: adenina, cytozyna, guanina i tymina. Jeśli porównamy dwa łańcuchy DNA przypadkowych przechodniów, będę w 99,9% takie same. Różnice polegają na tym, że w tych samych miejscach w łańcuchu występują różne zasady azotowe. Zasada azotowa jest składnikiem nukleotydu i na potrzeby tego tekstu te dwa terminy będę stosował zamiennie.

U zwierząt i roślin zwykle szukamy dłuższych fragmentów DNA, które mogą wystąpić w różnych wersjach, natomiast u ludzi najczęściej rozpatrujemy każdy z nukleotydów z osobna, a te, które przynajmniej u jednego procenta osób wyglądają inaczej niż u reszty, to tak zwane polimorfizmy pojedynczego nukleotydu (*Single Nucleotide Polymorphism*, SNP, “snip”).

Na poniższym rysunku zaprezentowano, jak zwykle wyglądają gen i SNP. Trzeba jednak odnotować, że genem może być również pojedynczy nukleotyd i wtedy te pojęcia są tożsame.

```{r echo = FALSE}
p <- readJPEG("gen_i_snp.jpg")
grid::grid.raster(p)
```

## Lokalizacja genów

Szukanie (lokalizacja) genów, to próba wskazania, które z nich są związane z interesującą nas cechą. “Związane” oznacza, że to, w jakiej wersji dany gen występuje, koreluje z cechą. Do czego nam to potrzebne? Dzięki takiej informacji możemy np. lepiej zrozumieć przyczynę choroby i tym samym opracować skuteczniejszy lek. Jesteśmy również w stanie znacznie szybciej oszacować ryzyko zachorowania i wcześniej zacząć terapię. U zwierząt, na przykład u krów, jeśli odkryjemy, które geny są odpowiedzialne na przykład za mleczność, możemy krzyżować tylko pewne osobniki. A jeśli chcemy, by w naszym sadzie rosły tylko słodkie owoce, zamiast przez kilkadziesiąt lat krzyżować różne odmiany, szukając optymalnych cech, możemy od razu użyć tych o odpowiednich parametrach.

## Kodowanie

Spróbujmy przetłumaczyć informację o genotypach danego osobnika na liczby. Oznaczyliśmy wcześniej allele genu przez *A* i *a*, co może wydać się nierozsądne, bo jaki symbol wybrać dla trzeciego allelu? Okazuje się, że taka sytuacja, tzn. wystąpienie trzeciej lub kolejnej wersji, jest na tyle mało prawdopodobna, że najczęściej w ogóle się tej szansy nie uwzględnia. Wynika to z faktu, że mutacja w jakimś miejscu DNA należy do rzadkości, więc kolejna w tym samym prawie się nie zdarza.

Moglibyśmy zatem kodować informację genetyczną przy pomocy jedynie dwóch liczb, ale trzeba jeszcze uwzględnić, że DNA znajduje się w chromosomach, które występują w parach. W odpowiadających sobie chromosomach nie mamy tych samych ciągów, gdyż jedną nić dziedziczymy po matce, drugą po ojcu. Zatem w danym miejscu ciągu DNA mamy trzy możliwości: *aa*, *aA* (ewentualnie *Aa*, ale kolejność nie ma znaczenia) lub *AA*.

Podsumowując, dla każdego osobnika możemy podać ciąg genotypów, kodowanych na przykład jako 0, 1 i 2, oraz wartość interesującej nas cechy. Poszczególne genotypy będą jakościowymi zmiennymi objaśniającymi, a cecha zmienną objaśnianą. Chcemy znaleźć te zmienne, które mają istotny wpływ na cechę.

Ponieważ będę analizował dane ludzkie, zmienne objaśniające to SNP. W dalszej części tekstu będę używał kolokwialnej nazwy “snipy”, żeby łatwiej się czytało.

## Dane

Wykorzystam dane [stąd](https://www.kaggle.com/datasets/seascape/snp-dataset-for-gwas). Nie są to rzeczywiste dane, ale bardzo do nich podobne. Cecha jest symulowana, dzięki czemu wiemy, jakie snipy rzeczywiście na nią wpływają. Wszystkich jest prawie pół miliona, a na dodatek obserwacji znacznie mniej niż zmiennych (1000). Jednym słowem, zadanie, które przed nami stoi, jest bardzo trudne. I jest to raczej typowa sytuacja dla tego typu danych.

## Model

Najprostszym podejściem byłoby skorelowanie każdego snipa z cechą, np. przy pomocy testu ANOVA (tzw. pojedyncze testy, *single tests*). Jest to jednak kiepskie rozwiązanie. Po pierwsze, mamy tu ogromny problem z wielokrotnym testowaniem i nie jest łatwo wybrać rozsądną korektę. Po drugie, wiadomo z genetyki, że wpływ na cechę ma pewna grupa snipów i nie jest to wpływ niezależny. Wykonując pojedyncze testy, zupełnie to ignorujemy. Możemy nie znaleźć któregoś snipa, bo jego istotny wpływ ujawnia się dopiero, gdy uwzględnimy wpływ innych. Oprócz tego z powodu wysokiej korelacji między bliskimi snipami, takie pojedyncze testy wskażą nam całą grupę bardzo podobnych snipów, zamiast wybrać reprezentanta.

Lepszym podejściem jest zastosowanie regresji liniowej i kryteriów wyboru modelu, jak AIC czy BIC. Problem w tym, że nie są one przystosowane do sytuacji, z którą mamy do czynienia. Zostały tak zaprojektowane, że w pewien sposób preferują modele, które składają się mniej więcej z połowy dostępnych zmiennych. Nawet jeśli świat tak wygląda (tzn. wpływ na cechę ma bardzo dużo snipów), to technicznie nie jesteśmy w stanie zbudować takiego modelu (zbyt mało obserwacji). Zwykle jednak zakłada się, że istotny wpływ na cechę ma kilkanaście, ewentualnie kilkadziesiąt snipów.

Jak powiedziałem, AIC i BIC nie wybiorą takiego modelu, gdyż kara za jego rozmiar (liczbę zmiennych) jest w nich zbyt niska. Dlatego zastosuję [kryterium mBIC](https://www.researchgate.net/publication/8469694_Modifying_the_Schwarz_Bayesian_Information_Criterion_to_Locate_Multiple_Interacting_Quantitative_Trait_Loci), w który jest ona znacznie większa.

Oprócz tego jest jeszcze problem techniczny, gdyż dane są bardzo duże. Wykorzystam pakiet [bigstep](https://cran.rstudio.com/web/packages/bigstep/vignettes/bigstep.html), który umożliwia zastosowanie procedury *stepwise selection* na bardzo dużych danych (oraz ma zaimplementowane kryterium mBIC). 

## Data science?

Tu krótka dygresja. Termin “data science” rozumie się na różne sposoby. Czasem jako po prostu analizę danych, w której nie wykraczamy poza standardowe procedury. I dobrze, jeśli problem tego nie wymaga, nie ma sensu na siłę stosować jakichś wyrafinowanych technik. Ale chyba słowo “science” najbardziej pasuje do tego, co robimy tutaj. Problem wymaga, byśmy najpierw wykonali pewien “research”. A z racji tego, że zadanie jest specyficzne, może się okazać, że rozwiązanie znajdziemy jedynie w publikacjach naukowych.

Co gorsze, ta odpowiedź może być tylko teoretyczna i następnie sami będziemy musieli stworzyć narzędzie, by móc przeprowadzić analizę. Akurat w naszym przypadku to narzędzie (pakiet w R) jest.

## Analiza

Ponieważ dane są duże (ok. 1 GB), nie wczytuję ich do RAM, ale wykorzystuję funkcję `read.big.matrix()` z pakietu `bigmemory` (`bigstep` współpracuje z tym pakietem).

```{r}
library("bigstep")
library("bigmemory")

X <- read.big.matrix("X.txt", sep = " ", type = "char", head = TRUE)
y <- read.table("y.txt")
```

Funkcja `prepare_data()` łączy dane X i Y, tworząc obiekt klasy `big`.

```{r}
snp_data <- prepare_data(y, X, verbose = FALSE) 
```

Snipów będę szukał przy pomocy poniższej procedury.

```{r cache = TRUE}
results <- snp_data %>% 
  reduce_matrix(minpv = 0.15) %>% 
  fast_forward(crit = bic, maxf = 70) %>% 
  multi_backward(crit = mbic) %>% 
  stepwise(crit = mbic)
```

Poniżej tłumaczę, na czym polegają poszczególne kroki.

### 1. Wstępna redukcja zmiennych

Zaczynam od ograniczenia liczby zmiennych (funkcja `reduce_matrix()`). Zostawiam tylko te snipy, które mogą korelować z cechą (p-wartość dla korelacji Pearsona mniejsza od 0,15). Oczywiście trzeba uważać, bo możemy tym sposobem usunąć ważnego snipa, którego wpływ daje się zauważyć dopiero, gdy w modelu uwzględnimy inne. Tego typu sytuacje się zdarzają, co pokażę pod koniec.

Ten próg 0,15 można oczywiście zwiększyć lub zmniejszyć, w zależności od tego, jak szybki mamy komputer i ile czasu możemy przeznaczyć na analizę.

Jak napisałem, wykorzystuję korelację Pearsona, co może wydawać się dość zaskakujące. Zmienne objaśniające są jakościowe i przyjmują trzy wartości (0, 1, 2). Lepszym podejściem byłby tu test ANOVA. Natomiast po pierwsze, te kategorie zmiennych mają pewien naturalny porządek (wersja typowa na obu chromosomach, nietypowa na jednym, nietypowa na dwóch). Wciąż to nie brzmi jak korelacja Pearsona, bo przecież związek między taką zmienną a cechą nie musi być liniowy. Ale podejdziemy do tego w ten sposób, bo tak będzie szybciej...

Kiepski argument? Po pierwsze, w problemach biznesowych czas jest bardzo ważnym czynnikiem. Jeśli budujemy model, musimy minimalizować nie tylko błąd, ale też koszt wykonania takiego modelu (a więc i czas). Po drugie, dalej i tak będę korzystał z regresji liniowej, także na relacje ze snipami będę patrzył liniowo. Po trzecie, naszym zadaniem jest nie tyle stworzyć dobry model (parametry jak najbliższe prawdzie), ale znaleźć istotne zmienne. A to zwykle jest prostsze: nawet jeśli nie założymy poprawnej postaci relacji między zmiennymi, jest duża szansa, że ją zauważymy (otrzymamy niską p-wartość).

### 2. Wstępny model

W drugim kroku stosuję szybką procedurę `fast_forward()`, która dołącza zmienne na podstawie klasycznego kryterium BIC. Działa ona na takiej zasadzie, że poprzedni krok (redukcja snipów) dodatkowo ustawia zmienne w kolejności od najmocniej do najsłabiej skorelowanej z cechą. Funkcja `fast_forward()` próbuje w tej kolejność dołączyć zmienne do modelu, o ile polepszają wartość kryterium BIC. Tzn. nie szuka NAJLEPSZEJ zmiennej minimalizującej BIC, ale dołącza każdą, która to kryterium zmniejsza — dzięki temu działa to bardzo szybko. Gdy zmiennych jest już odpowiednio dużo (u mnie 70), kończymy. Dzięki temu etapowi tanim kosztem dostajemy wstępny, dość duży model.

Teoretycznie można by już na tym etapie stosować mBIC, ale z powodu znacznie większej kary, nie zbudowalibyśmy odpowiednio dużego modelu.

### 3. Odrzucenie zmiennych

Przechodzę do procedury *backward* (funkcja `multi_backward()`). Stosuję ją w standardowy sposób, tzn. szukam najgorszej zmiennej i ją usuwam, o ile zmniejszy to wartość kryterium. Tu już korzystam z mBIC. Ta funkcja również jest szybka, bo procedura *backward* jest szybka: testujemy tylko zmienne, które są w modelu, a jest ich stosunkowo niewiele (w porównaniu do ogromnej liczby zmiennych poza modelem).

### 4. Stepwise

W końcu poruszam się w obu kierunkach (`stepwise()`). Ten etap jest potencjalnie najbardziej czasochłonny, bo krok *forward* jest trudny (jak pisałem wyżej). Choć może się zdarzyć, że już dzięki poprzednim etapom wybierzemy najlepszy model, wtedy `stepwise()` szybko się skończy.

## Wyniki

Możemy podsumować otrzymany model przy pomocy funkcji `summary()`

```{r}
summary(results)
```

Znaleźliśmy 24 snipy. Ponieważ cecha była symulowana, możemy sprawdzić, jak nam poszło, porównując znalezione snipy z prawdziwymi, podanymi w opisie danych. I okazuje się, że znaleźliśmy wszystkie. Niektóre na trochę innej pozycji (np. CH10_SNP445 zamiast CH10_SNP444), co wynika z bardzo wysokiej korelacji między tak bliskimi snipami.

Co interesujące, chociaż snip CH01_SNP19810 jest ważny (i ma bardzo niską p-wartość w modelu regresji), jest bardzo trudny do odnalezienia. Korelacja między nim a cechą jest bardzo niska:

```{r}
cor.test(X[, "CH01_SNP19810"], y[[1]])
```

P-wartość wynosi 0,106, więc jeśli parametr `minpv` w pierwszym kroku nie byłby odpowiednio duży, tego snipa w ogóle byśmy nie znaleźli! To pokazuje, jak w tego typu problemach ważny jest odpowiedni model.


